{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbx-l3scJLOA"
      },
      "source": [
        "# <center> Improved Arrhythmia Classification Using Select  </center>\n",
        "# <center>Morphological and Heart Rate Variability ECG Features. </center>\n",
        "## <center> Mark James Dunbar </center>\n",
        "\n",
        "<center>School of Electronic Engineering and Computer Science </center>\n",
        "<center>Queen Mary University of London </center>\n",
        "<center>London, United Kingdom </center>\n",
        "<center>ec21896@qmul.ac.uk </center>\n",
        "\n",
        "## <center> MSc Big Data Science - Final Project </center>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# <center>MSc Final Project Data Pre-Processing Notebook</center>\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, I will extract and pre-process the ECG data to be used in model experimentation and the final pipeline. The dataset can be found here (https://www.kaggle.com/datasets/nelsonsharma/ecg-lead-2-dataset-physionet-open-access), and is open access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHmarej30st-"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzHeP0lHO-b_",
        "outputId": "88443023-8890-4b9c-c603-1055b70443c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXi11-XpizNn"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import label\n",
        "from tqdm import tqdm, notebook\n",
        "\n",
        "def linebreak():\n",
        "  print('________________________________________________________________________________')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFRwp6HuuA1b"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiRHvUK9cXGn"
      },
      "source": [
        "Set the sample rate as a global variable, and the path of the raw ECG data to be extracted and pre-processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uosJdDUKi1dV"
      },
      "outputs": [],
      "source": [
        "# Dataset sample Rate of 128 Hz\n",
        "sample_rate = 128 \n",
        "\n",
        "# Path to data through google drive\n",
        "dataset_path = 'drive/MyDrive/data/db_npy'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kCOOqgtcRIE"
      },
      "source": [
        "Check out the standard PhysioNet annotations provided in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dI-ak4di6wv",
        "outputId": "97bf6f89-b18b-4665-acff-b350a6d366d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beat Annotations:      19\n",
            "Non-Beat Annotations:  22\n"
          ]
        }
      ],
      "source": [
        "standard_annotations = os.path.join(dataset_path,'annotations.txt')\n",
        "\n",
        "# Load the annotations file as a matrix\n",
        "annotations_txt = np.loadtxt(standard_annotations, dtype='str',delimiter=\"\\t\")\n",
        "\n",
        "annotations_b =  []\n",
        "annotations_n = []\n",
        "\n",
        "# Print a list of the annotations, and separate the beat and non-beat annotations\n",
        "for n in annotations_txt:\n",
        "  if n[1] == 'b':\n",
        "    annotations_b.append(n)\n",
        "  else:\n",
        "    annotations_n.append(n)\n",
        "\n",
        "print(\"Beat Annotations:     \",len(annotations_b))\n",
        "print(\"Non-Beat Annotations: \",len(annotations_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cJkXR58cj86"
      },
      "source": [
        "AAMI recommended classes. We'll map these to the PhysioNet standard annotations. PhysioNet non-beat annotations will be replaced with the \"No feature present\" label: `-`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vwhGlnPjdDZ"
      },
      "outputs": [],
      "source": [
        "# Note we have 7 classes here, but the F, X and Q classes will be removed from the dataset due to a small number of classes.\n",
        "\n",
        "N = 'N' # Normal (Non-Ectopic) Beats\n",
        "S = 'S' # Supra Ventricular Ectopic Beats\n",
        "V = 'V' # Ventricular Ectopic Beats\n",
        "F = 'F' # Fusion Beats\n",
        "Q = 'Q' # Unknown Beats\n",
        "X = 'X' # Unmapped\n",
        "\n",
        "remapping_dict = {\n",
        "              'N': N,\n",
        "              'R': N,\n",
        "              'B': N,\n",
        "              'L': N,\n",
        "              'A': S,\n",
        "              'a': S,\n",
        "              'n': S,\n",
        "              'J': S,\n",
        "              'S': S,\n",
        "              'j': S,\n",
        "              'e': S,\n",
        "              'V': V, \n",
        "              'r': V,\n",
        "              'E': V,\n",
        "              'F': F,\n",
        "              'f': _,\n",
        "              '/': _,\n",
        "              'Q': Q,\n",
        "              '?': Q,\n",
        "             }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjclll1CcxQ0"
      },
      "source": [
        "Class for extracting the data. Each record is saved as a pandas dataframe with the column 'voltage' for the actual ECG signal, and 'label' for the ECG AAMI labels corresponding to the indexes of the ECG signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFId7Uqijs9T"
      },
      "outputs": [],
      "source": [
        "class extractData:\n",
        "  def __init__(self, database_name, database_path, drop, remapping_dict, extract_to):\n",
        "    self.database_name = database_name\n",
        "    self.database_path = database_path\n",
        "    self.remapping_dict = remapping_dict\n",
        "    self.extract_to = extract_to\n",
        "    # Drop unwanted records\n",
        "    self.record_list = [x for x in np.loadtxt(os.path.join(database_path, database_name, 'RECORDS'), dtype='str',delimiter=\"\\n\") if x not in drop]\n",
        "    self._tidy()\n",
        "    self._extract()\n",
        "    \n",
        "  def _tidy(self):\n",
        "    linebreak()\n",
        "    print(f\"Beginning data extraction from {self.database_name}\")\n",
        "    for f in tqdm(os.listdir(self.extract_to), desc='Old Data Removal'):\n",
        "      if any(self.record_list) == f:\n",
        "        os.remove(os.path.join(self.extract_to, f))\n",
        "\n",
        "  def _extract(self):\n",
        "    \n",
        "    # Iterate over every record in the record list, extract voltages and labels as a single pandas dataframe\n",
        "    for record_index in tqdm(self.record_list, desc='Data Extraction '):\n",
        "      # Voltages\n",
        "      voltages_filename = f\"{record_index}_SIG_II.npy\"\n",
        "      voltages = np.load(os.path.join(self.database_path,self.database_name,voltages_filename))\n",
        "\n",
        "      # Annotations\n",
        "      annotations_filename = f\"{record_index}_BEAT.npy\"\n",
        "      annotations = np.load(os.path.join(self.database_path,self.database_name,annotations_filename))\n",
        "\n",
        "      # Generate an empty array of R-peak and Non-R peak labels (non R-peak = '_')\n",
        "      labels = np.full_like(voltages, '_', dtype='str')\n",
        "      labels_dict = {}\n",
        "      # Get a dictionary of beat locations\n",
        "      for i in annotations:\n",
        "        labels_dict[int(i[0])] = i[1]\n",
        "      # Map each beat to the empty labels array\n",
        "      for idx, m in enumerate(labels):\n",
        "        if idx in labels_dict:\n",
        "\n",
        "          # assign labels\n",
        "          labels[idx - 1] = labels_dict[idx]\n",
        "\n",
        "          # remap the labels\n",
        "          for key,value in self.remapping_dict.items():\n",
        "            if key in labels[idx - 1]:\n",
        "              labels[idx - 1] = labels[idx - 1].replace(key,value)\n",
        "\n",
        "      # Combine the data into a single pandas dataframe of exactly 30 minutes length\n",
        "      ecg_dataframe = pd.DataFrame(np.column_stack((voltages,labels)), columns=['voltage','label'])\n",
        "      ecg_dataframe['voltage'] = ecg_dataframe['voltage'].astype(float)\n",
        "      if len(ecg_dataframe) > 230400:\n",
        "        diff = len(ecg_dataframe) - 230400\n",
        "        ecg_dataframe.drop(ecg_dataframe.tail(diff).index, inplace = True)\n",
        "      elif len(ecg_dataframe) < 230400:\n",
        "        print(f\"ECG record too short for record {record_index}\")\n",
        "\n",
        "      # Pickle the dataframe in the extraction path\n",
        "      ecg_dataframe.to_pickle(os.path.join(self.extract_to,f\"{record_index}\"))\n",
        "\n",
        "    print(f\"\\nData successfully extracted from {self.database_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX9OJ2j6eDv8"
      },
      "source": [
        "## Extracting, Preprocessing and Saving the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54u5OlPvlY7i"
      },
      "outputs": [],
      "source": [
        "# Path to extract data to\n",
        "extract_to = \"drive/MyDrive/data/ECG_extracted_data\"\n",
        "\n",
        "# Databases to extract from\n",
        "databases = ['mitdb_npy', 'incartdb_npy', 'svdb_npy']\n",
        "\n",
        "# Records to drop due to unclassified beats\n",
        "records_to_drop = ['102', '104', '107', '207', '217']\n",
        "\n",
        "for database in databases:\n",
        "  extraction = extractData(database, dataset_path, records_to_drop, remapping_dict, extract_to)\n",
        "  linebreak()\n",
        "print(len(os.listdir(extract_to)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ProjectExtractionAndPreprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
